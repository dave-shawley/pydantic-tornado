{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Warning DO NOT USE THIS LIBRARY ANYWHERE THAT WILL AFFECT CUSTOMERS OR BUSINESSES I reserve the right to stop working on this, change the name, completely reorganize the library, and pretty much anything else at any time. This library brings some of the niceties of fastapi to Tornado applications. Instead of writing request handlers as stateful class instances, you write functions that use pydantic to define request and response bodies. This library takes care of the serialization and deserialization details for you. My goal is to write HTTP API servers that look a lot like FastAPI while reusing the pile of Tornado-based code that I'm already entrenched in. Goals of this library \u00b6 write Tornado request handlers as type annotated functions hide the serialization and deserialization details expose OpenAPI specification that is usable and correct Current status \u00b6 Extremely immature is a pretty good description. Incomplete and untested is another way to say it. I really want this to go past the vapourware stage, but it is little more than a proof-of-concept currently. Routing Request processing Response processing OpenAPI Named path parameter handling Positional path parameter handling Pydantic parameter serialization Injection of Tornado handling context Pydantic request body deserialization JSON responses from simple types Proactive content negotiation Pydantic response serialization OpenAPI schema from basic types OpenAPI schema from Pydantic OpenAPI schema generation for operations Handler for generated OpenAPI JSON Sales pitch \u00b6 Consider the following naive implementation of a simple CRUD handler. import json import tornado.web class WidgetAPI ( tornado . web . RequestHandler ): async def get ( self , widget_id : str ) -> None : row = await some_database . execute ( 'SELECT id, name, description, created_at' ' FROM public.widgets' ' WHERE id = %(widget_id)s ' , { 'widget_id' : widget_id } ) if not row : raise tornado . web . HTTPError ( 404 ) self . write ( json . dumps ( row )) async def post ( self ) -> None : details = json . loads ( self . request . body . decode ( 'utf-8' )) row = await some_database . execute ( 'INSERT INTO public.widgets (name, description,' ' created_at)' ' VALUES ( %(name)s , %(description)s ,' ' CURRENT_TIMESTAMP)' ' RETURNING id, name, description' , { 'name' : details [ 'name' ], 'description' : details [ 'description' ]} ) self . write ( json . dumps ( row )) Now, quickly answer a few questions: is the response body consistent between methods? what is the type of id ? We can answer the first of the questions but not the second. What if we could rewrite this example as: import datetime import uuid import pydantic class CreateWidgetRequest ( pydantic . BaseModel ): name : str description : str class Widget ( CreateWidgetRequest ): id : pydantic . UUID4 name : str description : str created_at : datetime . datetime async def get_widget ( widget_id : uuid . UUID ) -> Widget | None : row = await some_database . execute ( 'SELECT id, name, description, created_at' ' FROM public.widgets' ' WHERE id = %(widget_id)s ' , { 'widget_id' : widget_id } ) if row : return Widget . model_validate ( row ) async def create_widget ( widget_details : CreateWidgetRequest ) -> Widget : row = await some_database . execute ( 'INSERT INTO public.widgets (name, description,' ' created_at)' ' VALUES ( %(name)s , %(description)s ,' ' CURRENT_TIMESTAMP)' ' RETURNING id, name, description, created_at' , widget_details . dict () ) return Widget . model_validate ( row ) Yes ... there is a bit more code there. Sorry / not sorry. Oh, there is a missing chunk in both snippets -- adding the routes to the application . I'll skip the traditional example. Here is what I am envisioning for the new code. from tornado import web from pydantictornado import routing class Application ( web . Application ): def __init__ ( self , ** settings ): super () . __init__ ([ routing . Route ( '/widgets' , post = create_widget ), routing . Route ( '/widgets/(?P<widget_id>.*)' , get = get_widget ), ], ** settings ) Instead of having separate tornado.web.RequestHandler classes for each HTTP route, this library exposes a function that returns a route that will send GET , POST , et al. requests to a resource to our functions. The machinery inspects the type annotations on the callables and uses them to deserialize request bodies before calling the function. Similarly, return annotations dictate the response types which are serialized using the standard pydantic methods. More than syntax sweetening? \u00b6 I agree that it is easy to think of this as simple syntactical sugar but there is more to it than that. It is moving the request and response structures into well-defined classes. This also makes it possible to generate OpenAPI specifications directly from the annotated functions. No need to embed OpenAPI specs in docstrings or write cumbersome YAML files. And ... yes ... I am stealing the concepts from FastAPI. After working with FastAPI for a little while, I decided that the structure is really pleasant to work with. Not having the deal with serialization or keeping API documentation in sync is a great boon to developer productivity. Injecting parameters based on type annotations is a little magical at first -- think of passing light through a prism as opposed to a natural double rainbow. It quickly becomes second nature and does the right thing. It can be simpler than what FastAPI uses by constraining the functionality slightly. You'll see what I'm talking about.","title":"Pydantic-tornado"},{"location":"#goals-of-this-library","text":"write Tornado request handlers as type annotated functions hide the serialization and deserialization details expose OpenAPI specification that is usable and correct","title":"Goals of this library"},{"location":"#current-status","text":"Extremely immature is a pretty good description. Incomplete and untested is another way to say it. I really want this to go past the vapourware stage, but it is little more than a proof-of-concept currently. Routing Request processing Response processing OpenAPI Named path parameter handling Positional path parameter handling Pydantic parameter serialization Injection of Tornado handling context Pydantic request body deserialization JSON responses from simple types Proactive content negotiation Pydantic response serialization OpenAPI schema from basic types OpenAPI schema from Pydantic OpenAPI schema generation for operations Handler for generated OpenAPI JSON","title":"Current status"},{"location":"#sales-pitch","text":"Consider the following naive implementation of a simple CRUD handler. import json import tornado.web class WidgetAPI ( tornado . web . RequestHandler ): async def get ( self , widget_id : str ) -> None : row = await some_database . execute ( 'SELECT id, name, description, created_at' ' FROM public.widgets' ' WHERE id = %(widget_id)s ' , { 'widget_id' : widget_id } ) if not row : raise tornado . web . HTTPError ( 404 ) self . write ( json . dumps ( row )) async def post ( self ) -> None : details = json . loads ( self . request . body . decode ( 'utf-8' )) row = await some_database . execute ( 'INSERT INTO public.widgets (name, description,' ' created_at)' ' VALUES ( %(name)s , %(description)s ,' ' CURRENT_TIMESTAMP)' ' RETURNING id, name, description' , { 'name' : details [ 'name' ], 'description' : details [ 'description' ]} ) self . write ( json . dumps ( row )) Now, quickly answer a few questions: is the response body consistent between methods? what is the type of id ? We can answer the first of the questions but not the second. What if we could rewrite this example as: import datetime import uuid import pydantic class CreateWidgetRequest ( pydantic . BaseModel ): name : str description : str class Widget ( CreateWidgetRequest ): id : pydantic . UUID4 name : str description : str created_at : datetime . datetime async def get_widget ( widget_id : uuid . UUID ) -> Widget | None : row = await some_database . execute ( 'SELECT id, name, description, created_at' ' FROM public.widgets' ' WHERE id = %(widget_id)s ' , { 'widget_id' : widget_id } ) if row : return Widget . model_validate ( row ) async def create_widget ( widget_details : CreateWidgetRequest ) -> Widget : row = await some_database . execute ( 'INSERT INTO public.widgets (name, description,' ' created_at)' ' VALUES ( %(name)s , %(description)s ,' ' CURRENT_TIMESTAMP)' ' RETURNING id, name, description, created_at' , widget_details . dict () ) return Widget . model_validate ( row ) Yes ... there is a bit more code there. Sorry / not sorry. Oh, there is a missing chunk in both snippets -- adding the routes to the application . I'll skip the traditional example. Here is what I am envisioning for the new code. from tornado import web from pydantictornado import routing class Application ( web . Application ): def __init__ ( self , ** settings ): super () . __init__ ([ routing . Route ( '/widgets' , post = create_widget ), routing . Route ( '/widgets/(?P<widget_id>.*)' , get = get_widget ), ], ** settings ) Instead of having separate tornado.web.RequestHandler classes for each HTTP route, this library exposes a function that returns a route that will send GET , POST , et al. requests to a resource to our functions. The machinery inspects the type annotations on the callables and uses them to deserialize request bodies before calling the function. Similarly, return annotations dictate the response types which are serialized using the standard pydantic methods.","title":"Sales pitch"},{"location":"#more-than-syntax-sweetening","text":"I agree that it is easy to think of this as simple syntactical sugar but there is more to it than that. It is moving the request and response structures into well-defined classes. This also makes it possible to generate OpenAPI specifications directly from the annotated functions. No need to embed OpenAPI specs in docstrings or write cumbersome YAML files. And ... yes ... I am stealing the concepts from FastAPI. After working with FastAPI for a little while, I decided that the structure is really pleasant to work with. Not having the deal with serialization or keeping API documentation in sync is a great boon to developer productivity. Injecting parameters based on type annotations is a little magical at first -- think of passing light through a prism as opposed to a natural double rainbow. It quickly becomes second nature and does the right thing. It can be simpler than what FastAPI uses by constraining the functionality slightly. You'll see what I'm talking about.","title":"More than syntax sweetening?"},{"location":"todo/","text":"This is simply a collection of things that I would like this library to do that it doesn't currently do. I might eventually organize these into milestones or whatever, but I'm not that organized at the moment. OpenAPI \u00b6 document parameter annotations in some sensible way add header support via injections shared schema for pydantic models response models figure out how to describe status codes Injections \u00b6 add header support add authorization model Other \u00b6 error handling hooks support for PEP-695 type aliases","title":"TODO"},{"location":"todo/#openapi","text":"document parameter annotations in some sensible way add header support via injections shared schema for pydantic models response models figure out how to describe status codes","title":"OpenAPI"},{"location":"todo/#injections","text":"add header support add authorization model","title":"Injections"},{"location":"todo/#other","text":"error handling hooks support for PEP-695 type aliases","title":"Other"},{"location":"adr/","text":"I am trying to capture the important decisions as I write this library here so that they are not lost to the sands of time. Michael Nygard describes the need for this much better than I ever could in Documenting Architecture Decisions . The graph below shows the decisions and any relationships between them. graph TD adr0001-record-architecture-decisions[Record architecture decisions] click adr0001-record-architecture-decisions \"adr/0001-record-architecture-decisions/\" _blank adr0001-record-architecture-decisions:::mermaid-accepted adr0001-record-architecture-decisions:::mermaid-common adr0002-generate-documentation-using-mkdocs[Generate documentation using mkdocs] click adr0002-generate-documentation-using-mkdocs \"adr/0002-generate-documentation-using-mkdocs/\" _blank adr0002-generate-documentation-using-mkdocs:::mermaid-accepted adr0002-generate-documentation-using-mkdocs:::mermaid-common adr0003-type-annotations[Rely on Python type annotations] click adr0003-type-annotations \"adr/0003-type-annotations/\" _blank adr0003-type-annotations:::mermaid-accepted adr0003-type-annotations:::mermaid-common classDef mermaid-draft fill:#a3a3a3; classDef mermaid-common color:#595959; classDef mermaid-proposed fill:#b6d8ff; classDef mermaid-common color:#595959; classDef mermaid-accepted fill:#b4eda0; classDef mermaid-common color:#595959; classDef mermaid-rejected fill:#ffd5d1; classDef mermaid-common color:#595959; classDef mermaid-superseded fill:#ffebb6; classDef mermaid-common color:#595959;","title":"Architecture decisions"},{"location":"adr/0001-record-architecture-decisions/","text":"Record architecture decisions \u00b6 Context \u00b6 The rationale behind using a programming technique or adopting a specific tool is usually lost as soon as the decision is enshrined in code. It is too easy to disregard what was a conscious decision made in the past. We make decisions every day. Some can be changed without substantial consequences. The ones that have substantial consequences need to be recorded somewhere so that they are recognized as impactful decisions. See Michael Nygard's insightful article Documenting Architecture Decisions for additional thoughts on the subject. Decision \u00b6 record architecturally significant decisions using lightweight Architecture Decision Records include the ADRs in our documentation suite Each decision is recorded by a single file in the doc/adr directory using the following template: --- adr: author: Your Name Here created: dd-Mmm-YYYY status: draft | proposed | rejected | accepted | superseded --- # Title ## Context Describe why you felt the need to make a decision. ## Decision The decision including important details. ## Consequences The known ramifications of making this decision including what is easier to do or what is more difficult to do. Make sure to include the ramifications of changing this decision in the future. This format is used in conjunction with the mkdocs-material-adr plugin to include the records in our documentation suite. Consequences \u00b6 Making important decisions is an intentional action since they require sitting down and writing out your rationale. This means that making a decision takes more time. The same is also true about changing a decision once it has been enshrined in an ADR. There is a lack of decent automation here. [mkdocs-material-adr-plugin] makes a very nice graph of ADR relationships, but you are still required to add the document to mkdocs.yaml . I also could not find a \"decent\" tool to generate ADRs that took a template. This creates slightly more friction to documenting decisions than I would prefer.","title":"Record architecture decisions"},{"location":"adr/0001-record-architecture-decisions/#record-architecture-decisions","text":"","title":"Record architecture decisions"},{"location":"adr/0001-record-architecture-decisions/#context","text":"The rationale behind using a programming technique or adopting a specific tool is usually lost as soon as the decision is enshrined in code. It is too easy to disregard what was a conscious decision made in the past. We make decisions every day. Some can be changed without substantial consequences. The ones that have substantial consequences need to be recorded somewhere so that they are recognized as impactful decisions. See Michael Nygard's insightful article Documenting Architecture Decisions for additional thoughts on the subject.","title":"Context"},{"location":"adr/0001-record-architecture-decisions/#decision","text":"record architecturally significant decisions using lightweight Architecture Decision Records include the ADRs in our documentation suite Each decision is recorded by a single file in the doc/adr directory using the following template: --- adr: author: Your Name Here created: dd-Mmm-YYYY status: draft | proposed | rejected | accepted | superseded --- # Title ## Context Describe why you felt the need to make a decision. ## Decision The decision including important details. ## Consequences The known ramifications of making this decision including what is easier to do or what is more difficult to do. Make sure to include the ramifications of changing this decision in the future. This format is used in conjunction with the mkdocs-material-adr plugin to include the records in our documentation suite.","title":"Decision"},{"location":"adr/0001-record-architecture-decisions/#consequences","text":"Making important decisions is an intentional action since they require sitting down and writing out your rationale. This means that making a decision takes more time. The same is also true about changing a decision once it has been enshrined in an ADR. There is a lack of decent automation here. [mkdocs-material-adr-plugin] makes a very nice graph of ADR relationships, but you are still required to add the document to mkdocs.yaml . I also could not find a \"decent\" tool to generate ADRs that took a template. This creates slightly more friction to documenting decisions than I would prefer.","title":"Consequences"},{"location":"adr/0002-generate-documentation-using-mkdocs/","text":"Generate documentation using mkdocs \u00b6 Context \u00b6 ReStructuredText and sphinx have been the standard tools that I've used for writing Python documentation for many years. I've come to recognize that Python developers don't document their software and part of it may be that the toolkit feels foreign. Over the same period of time, Markdown has become the defacto standard for documentation in all software circles. Maybe switching to a more popular format will help me and others write awesome documentation for our projects. So... switching to Markdown is a big decision. The next decision is to figure out which toolchain to use since there are several out there. Let's start with some requirements: simple and unobtrusive works well with the tools that I use -- PyCharm, simple text editors, pyproject.toml workflow doesn't require a non-python toolchain ability to publish documentation simply and easily creates an ergonomic documentation site streamlines the documentation process Decision \u00b6 document this software package using Markdown use mkdocs to build the documentation suite Consequences \u00b6 mkdocs was not an easy choice though it is a long-lived project with a wealth of \"plugins\". It also has more than its share of legacy. I'm a little cautious about that part... the other contender was Docusaurus which I decided was too complex for simple documentation. You might think that I would be concerned about switching away from sphinx, but I'm really not too concerned. Very few developers understand ReStructuredText well and even fewer care to learn the sphinx ecosystem well-enough to write documentation as is.","title":"Generate documentation using mkdocs"},{"location":"adr/0002-generate-documentation-using-mkdocs/#generate-documentation-using-mkdocs","text":"","title":"Generate documentation using mkdocs"},{"location":"adr/0002-generate-documentation-using-mkdocs/#context","text":"ReStructuredText and sphinx have been the standard tools that I've used for writing Python documentation for many years. I've come to recognize that Python developers don't document their software and part of it may be that the toolkit feels foreign. Over the same period of time, Markdown has become the defacto standard for documentation in all software circles. Maybe switching to a more popular format will help me and others write awesome documentation for our projects. So... switching to Markdown is a big decision. The next decision is to figure out which toolchain to use since there are several out there. Let's start with some requirements: simple and unobtrusive works well with the tools that I use -- PyCharm, simple text editors, pyproject.toml workflow doesn't require a non-python toolchain ability to publish documentation simply and easily creates an ergonomic documentation site streamlines the documentation process","title":"Context"},{"location":"adr/0002-generate-documentation-using-mkdocs/#decision","text":"document this software package using Markdown use mkdocs to build the documentation suite","title":"Decision"},{"location":"adr/0002-generate-documentation-using-mkdocs/#consequences","text":"mkdocs was not an easy choice though it is a long-lived project with a wealth of \"plugins\". It also has more than its share of legacy. I'm a little cautious about that part... the other contender was Docusaurus which I decided was too complex for simple documentation. You might think that I would be concerned about switching away from sphinx, but I'm really not too concerned. Very few developers understand ReStructuredText well and even fewer care to learn the sphinx ecosystem well-enough to write documentation as is.","title":"Consequences"},{"location":"adr/0003-type-annotations/","text":"Rely on Python type annotations \u00b6 Context \u00b6 Type annotations are becoming increasingly popular in the Python community. The FastAPI framework embraced using annotations to describe how request information is deserialized. It also relied on Pydantic to handle the deserialization of request data as well as the serialization of response data. I rally appreciated this after a few years writing HTTP APIs in tornado . The need for serialization helpers becomes obvious pretty quickly and led me to create the sprockets.mixins.mediatype library that implements proactive content negotiation. Unfortunately the mediatype library implements both content negotiation via [accept] & [content-type] headers and the serialization of representations. It would be nice to decouple these aspects and make \"typed\" request handlers a reality in the process. Decision \u00b6 We can use type annotations to handle a number of different cross-cutting aspects. Use functions for request handling \u00b6 Moving the request handling logic out of tornado.web.RequestHandler.get() and its relatives and into freestanding functions puts you in control of the request and response data types. The framework can look at your annotations and \"do the right thing\" when it comes to making sure that your code is getting what it expects. Inject \"state\" from the handler using annotated parameters \u00b6 Tornado RequestHandler s and Application s have some state and functionality that request handling logic needs. Instead of creating new interfaces for adding headers or setting specific response codes, I decided to use the interfaces that already exist. You simply add a parameter annotated as a tornado.web.RequestHandler to gain access to the methods on the request handler class. Similarly, a parameter annotated with tornado.web.Application will receive the application instance. Hide (de-)serialization details \u00b6 I'm going to take a similar approach to the one that I took with sprockets.mixins.mediatype when it comes to the low-level serialization and deserialization. In fact, I might use the library for its content handlers alone. I want this library to handle deserializing the incoming request based on the content-type and serialize the response as well. The difference is that I want to take advantage of pydantic for values that are not \"basic\" (eg, list , dict , str , int , float ). I also want to use it for its constrained type management. Let pydantic handle semantic details \u00b6 Relying on pydantic for deserializing request bodies and serializing responses places the semantic interpretation of content outside the content-type management. Pydantic takes care of knowing that the modified_at field in a request is an ISO-8601 formatted date-time. The content-type management portion is responsible for translating a binary msgpack message into a Python dict containing primitive types. Consequences \u00b6 This approach places a large emphasis on type annotation processing of user-supplied functions. Interpreting the annotations on a function is pretty messy today ... even in Python 3.12. There are several different ways to retrieve type information from an annotated function and they differ slightly. This will make the implementation interesting and, perhaps, even brittle. The universe of typing-related PEPs is also a moving target. I'm betting on it being more well-behaved than the JSON schema tooling and OpenAPI specifications. Rejected alternatives \u00b6 Parsing docstrings \u00b6 The ability to parse information from structured docstrings using a well-known format (eg, sphinx) is one way to figure out what a request handler method expects. I didn't like this option since it either tied me to one docstring format or left me writing different parsers and coming up with an intermediate format. The intermediate format ends up looking a lot like type annotations or an OpenAPI specification. Tying an OpenAPI specification to endpoints \u00b6 Another option is to describe the endpoints in an OpenAPI specification and use the embedded JSON schema descriptions to validate requests and process responses. This approach is possible and would work relatively well if it weren't for one glaring problem -- JSON schema descriptions do not easily map to the Python type system... and I am aware that one of the goals of this library is to generate OpenAPI . This problem can be worked around by coming up with a mapping definition from arbitrary JSON schema to Python instances in the most recent OpenAPI specification (3.1.0). I ultimately stepped away from this approach for a few reasons that are important to understand: OpenAPI is an ever-changing specification. The 3.0 version used JSON schema for most things but not everything. This was rectified in 3.1 at the expense of having to rewrite the usage of nullable in your specification. The next revision looks to be a drastic simplification which will again require rewriting your specification. Maybe after the specifications normalize this can be made to work. JSON schema definitions and the Python type system don't see eye-to-eye. This is another thing that is slowly improving through revisions of JSON schema itself. Yes, it is a moving target as well. Consider the difference between lists and tuples. Python lists are meant to be extended with new values. Tuples, however, tend to have a fixed and known length. Describing a tuple containing an integer and two strings in JSON schema is a little more difficult than you might expect. Engineers are not great at writing precise OpenAPI descriptions. It is a little easier to write precise type annotations since they are at least still Python and good tooling exists for checking your annotations. An OpenAPI specification and the Python type system are different and they should be . This is more of a idealogical argument. I truly believe that writing an OpenAPI specification that describes the API and its intended usage should concentrate on making the specification useful for documentation and machine-readable validation. Letting Python type system details or implementation details leak into the specification muddies it plain and simple.","title":"Rely on Python type annotations"},{"location":"adr/0003-type-annotations/#rely-on-python-type-annotations","text":"","title":"Rely on Python type annotations"},{"location":"adr/0003-type-annotations/#context","text":"Type annotations are becoming increasingly popular in the Python community. The FastAPI framework embraced using annotations to describe how request information is deserialized. It also relied on Pydantic to handle the deserialization of request data as well as the serialization of response data. I rally appreciated this after a few years writing HTTP APIs in tornado . The need for serialization helpers becomes obvious pretty quickly and led me to create the sprockets.mixins.mediatype library that implements proactive content negotiation. Unfortunately the mediatype library implements both content negotiation via [accept] & [content-type] headers and the serialization of representations. It would be nice to decouple these aspects and make \"typed\" request handlers a reality in the process.","title":"Context"},{"location":"adr/0003-type-annotations/#decision","text":"We can use type annotations to handle a number of different cross-cutting aspects.","title":"Decision"},{"location":"adr/0003-type-annotations/#use-functions-for-request-handling","text":"Moving the request handling logic out of tornado.web.RequestHandler.get() and its relatives and into freestanding functions puts you in control of the request and response data types. The framework can look at your annotations and \"do the right thing\" when it comes to making sure that your code is getting what it expects.","title":"Use functions for request handling"},{"location":"adr/0003-type-annotations/#inject-state-from-the-handler-using-annotated-parameters","text":"Tornado RequestHandler s and Application s have some state and functionality that request handling logic needs. Instead of creating new interfaces for adding headers or setting specific response codes, I decided to use the interfaces that already exist. You simply add a parameter annotated as a tornado.web.RequestHandler to gain access to the methods on the request handler class. Similarly, a parameter annotated with tornado.web.Application will receive the application instance.","title":"Inject \"state\" from the handler using annotated parameters"},{"location":"adr/0003-type-annotations/#hide-de-serialization-details","text":"I'm going to take a similar approach to the one that I took with sprockets.mixins.mediatype when it comes to the low-level serialization and deserialization. In fact, I might use the library for its content handlers alone. I want this library to handle deserializing the incoming request based on the content-type and serialize the response as well. The difference is that I want to take advantage of pydantic for values that are not \"basic\" (eg, list , dict , str , int , float ). I also want to use it for its constrained type management.","title":"Hide (de-)serialization details"},{"location":"adr/0003-type-annotations/#let-pydantic-handle-semantic-details","text":"Relying on pydantic for deserializing request bodies and serializing responses places the semantic interpretation of content outside the content-type management. Pydantic takes care of knowing that the modified_at field in a request is an ISO-8601 formatted date-time. The content-type management portion is responsible for translating a binary msgpack message into a Python dict containing primitive types.","title":"Let pydantic handle semantic details"},{"location":"adr/0003-type-annotations/#consequences","text":"This approach places a large emphasis on type annotation processing of user-supplied functions. Interpreting the annotations on a function is pretty messy today ... even in Python 3.12. There are several different ways to retrieve type information from an annotated function and they differ slightly. This will make the implementation interesting and, perhaps, even brittle. The universe of typing-related PEPs is also a moving target. I'm betting on it being more well-behaved than the JSON schema tooling and OpenAPI specifications.","title":"Consequences"},{"location":"adr/0003-type-annotations/#rejected-alternatives","text":"","title":"Rejected alternatives"},{"location":"adr/0003-type-annotations/#parsing-docstrings","text":"The ability to parse information from structured docstrings using a well-known format (eg, sphinx) is one way to figure out what a request handler method expects. I didn't like this option since it either tied me to one docstring format or left me writing different parsers and coming up with an intermediate format. The intermediate format ends up looking a lot like type annotations or an OpenAPI specification.","title":"Parsing docstrings"},{"location":"adr/0003-type-annotations/#tying-an-openapi-specification-to-endpoints","text":"Another option is to describe the endpoints in an OpenAPI specification and use the embedded JSON schema descriptions to validate requests and process responses. This approach is possible and would work relatively well if it weren't for one glaring problem -- JSON schema descriptions do not easily map to the Python type system... and I am aware that one of the goals of this library is to generate OpenAPI . This problem can be worked around by coming up with a mapping definition from arbitrary JSON schema to Python instances in the most recent OpenAPI specification (3.1.0). I ultimately stepped away from this approach for a few reasons that are important to understand: OpenAPI is an ever-changing specification. The 3.0 version used JSON schema for most things but not everything. This was rectified in 3.1 at the expense of having to rewrite the usage of nullable in your specification. The next revision looks to be a drastic simplification which will again require rewriting your specification. Maybe after the specifications normalize this can be made to work. JSON schema definitions and the Python type system don't see eye-to-eye. This is another thing that is slowly improving through revisions of JSON schema itself. Yes, it is a moving target as well. Consider the difference between lists and tuples. Python lists are meant to be extended with new values. Tuples, however, tend to have a fixed and known length. Describing a tuple containing an integer and two strings in JSON schema is a little more difficult than you might expect. Engineers are not great at writing precise OpenAPI descriptions. It is a little easier to write precise type annotations since they are at least still Python and good tooling exists for checking your annotations. An OpenAPI specification and the Python type system are different and they should be . This is more of a idealogical argument. I truly believe that writing an OpenAPI specification that describes the API and its intended usage should concentrate on making the specification useful for documentation and machine-readable validation. Letting Python type system details or implementation details leak into the specification muddies it plain and simple.","title":"Tying an OpenAPI specification to endpoints"},{"location":"reference/","text":"Note I haven't written much documentation around this yet. At least you have the source code.","title":"Reference"},{"location":"reference/api/","text":"Application interface \u00b6 pydantictornado.routing.Route \u00b6 Bases: URLSpec Source code in src/pydantictornado/routing.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 class Route ( tornado . routing . URLSpec ): _implementations : dict [ str , request_handling . RequestMethod ] def __init__ ( self , pattern : str | re . Pattern [ str ], ** kwargs : request_handling . RequestMethod | object , ) -> None : self . _implementations = {} target_kwargs = {} for name , value in kwargs . items (): if name . upper () in HTTP_METHOD_NAMES : if not util . is_coroutine_function ( value ): raise errors . CoroutineRequiredError ( value ) self . _implementations [ name . upper ()] = value target_kwargs [ name ] = value if not self . _implementations : raise errors . NoHttpMethodsDefinedError if isinstance ( pattern , str ): pattern = re . compile ( pattern . removesuffix ( '$' ) + '$' ) path_types : dict [ str , Converter ] = {} path_groups = pattern . groupindex if path_groups : target_kwargs [ 'path_types' ] = path_types for impl in self . _implementations . values (): _process_path_parameters ( impl , pattern , path_types ) super () . __init__ ( pattern , handler = request_handling . RequestHandler , kwargs = target_kwargs , ) @property def implementations ( self , ) -> typing . Iterator [ tuple [ str , request_handling . RequestMethod ]]: yield from self . _implementations . items () pydantictornado.request_handling.RequestHandler \u00b6 Bases: RequestHandler Source code in src/pydantictornado/request_handling.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class RequestHandler ( web . RequestHandler ): implementations : dict [ str , RequestMethod ] logger : logging . Logger __initialization_failure : Exception | None __path_coercions : dict [ str , PathCoercion ] # The following annotations help out type checkers. The # methods are bound in `initialize()`. delete : RequestMethod # type: ignore[assignment] # signature mismatch get : RequestMethod # type: ignore[assignment] # signature mismatch head : RequestMethod # type: ignore[assignment] # signature mismatch options : RequestMethod # type: ignore[assignment] # signature mismatch patch : RequestMethod # type: ignore[assignment] # signature mismatch post : RequestMethod # type: ignore[assignment] # signature mismatch put : RequestMethod # type: ignore[assignment] # signature mismatch def initialize ( self , * , path_types : dict [ str , PathCoercion ] | None = None , ** kwargs : object , ) -> None : self . __initialization_failure = None self . logger = util . get_logger_for ( self ) self . implementations = {} self . __path_coercions = collections . defaultdict ( identity_transform_factory ) if path_types : self . __path_coercions . update ( path_types ) for http_method in self . SUPPORTED_METHODS : key = http_method . lower () func = kwargs . pop ( key , util . UNSPECIFIED ) if func is not util . UNSPECIFIED : if not util . is_coroutine_function ( func ): self . logger . critical ( 'implementation method for %r is not a co-routine' , key ) self . __initialization_failure = web . HTTPError ( 500 ) self . implementations [ http_method ] = typing . cast ( RequestMethod , func ) setattr ( self , key , self . _handle_request ) self . SUPPORTED_METHODS = tuple ( self . implementations . keys ()) # type: ignore[assignment] super () . initialize ( ** kwargs ) async def _handle_request ( self , ** path_kwargs : str ) -> None : if self . __initialization_failure is not None : raise self . __initialization_failure if self . request . method is None : raise web . HTTPError ( 500 ) # pragma: nocover -- should not happen! func = self . implementations [ self . request . method ] sig = inspect . signature ( func , eval_str = True ) try : kwargs = { name : self . __path_coercions [ name ]( value ) for name , value in path_kwargs . items () } except ValueError as error : self . logger . error ( 'failed to process path parameter for %s : %s ' , func , error ) raise web . HTTPError ( 400 ) from None self . __handle_injections ( sig . parameters , kwargs ) result = await func ( ** kwargs ) if result is not None or sig . return_annotation == ReturnsNone : self . send_response ( result ) def send_response ( self , body : ResponseType | None , * _args : object , ** _kwargs : object ) -> None : self . set_header ( 'content-type' , 'application/json; charset=\"UTF-8\"' ) self . write ( json . dumps ( body , default = util . json_serialize_hook ) . encode ( 'utf-8' ) ) def __handle_injections ( self , annotations : collections . abc . Mapping [ str , inspect . Parameter ], kwargs : dict [ str , object ], ) -> None : logger = util . get_logger_for ( self ) mapping = util . ClassMapping [ object ]( { tornado . httputil . HTTPServerRequest : self . request , tornado . web . Application : self . application , tornado . web . RequestHandler : self , } ) for name , param in annotations . items (): logger . debug ( 'processing %s -> annotation= %r origin= %r ' , name , param . annotation , typing . get_origin ( param . annotation ), ) if typing . get_origin ( param . annotation ) is None : if issubclass ( param . annotation , pydantic . BaseModel ): kwargs [ name ] = param . annotation . model_validate_json ( self . request . body ) else : value = mapping . get ( param . annotation , util . UNSPECIFIED ) if value is not util . UNSPECIFIED : kwargs [ name ] = value logger . debug ( 'kwargs[ %s ] <- %r ' , name , kwargs . get ( name , util . Unspecified ()) ) Useful utilities \u00b6 Types and type helpers \u00b6 pydantictornado.util.FieldOmittingMixin \u00b6 Bases: BaseModel Mix this into pydantic models to omit None fields The fields named in OMIT_IF_NONE will be ignored during serialization if their value is None . The fields named in OMIT_IF_EMPTY will be ignored during serialization if their value is empty . This is only relevant for instances that implement collections.abc.Sized . Source code in src/pydantictornado/util.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class FieldOmittingMixin ( pydantic . BaseModel ): \"\"\"Mix this into pydantic models to omit `None` fields The fields named in `OMIT_IF_NONE` will be ignored during serialization if their value is `None`. The fields named in `OMIT_IF_EMPTY` will be ignored during serialization if their value is *empty*. This is only relevant for instances that implement [collections.abc.Sized][]. \"\"\" OMIT_IF_EMPTY : typing . ClassVar [ tuple [ str , ... ]] = () OMIT_IF_NONE : typing . ClassVar [ tuple [ str , ... ]] = () @pydantic . model_serializer ( mode = 'wrap' ) def _omit_fields ( self , handler : typing . Callable [ [ typing . Self , pydantic . SerializationInfo ], dict [ str , object ] ], info : pydantic . SerializationInfo , ) -> dict [ str , object ]: result = handler ( self , info ) for name in self . model_fields : value = result . get ( name , None ) empty = isinstance ( value , collections . abc . Sized ) and not len ( value ) if ( name in self . OMIT_IF_NONE and value is None ) or ( name in self . OMIT_IF_EMPTY and empty ): result . pop ( name ) return result pydantictornado.util.HasIsoFormat \u00b6 Bases: Protocol Runtime checkable protocol that detects the isoformat method Source code in src/pydantictornado/util.py 259 260 261 262 263 264 @typing . runtime_checkable class HasIsoFormat ( typing . Protocol ): \"\"\"Runtime checkable protocol that detects the isoformat method\"\"\" def isoformat ( self , spec : str = ... ) -> str : ... pydantictornado.util.Unspecified \u00b6 Simple type that is never true Source code in src/pydantictornado/util.py 30 31 32 33 34 35 36 37 class Unspecified : \"\"\"Simple type that is never true\"\"\" def __bool__ ( self ) -> bool : # pragma: nocover # note that this method may or may not be necessary based # on the implementation ... I'm leaving it here to avoid # adding and removing it constantly return False pydantictornado . util . UNSPECIFIED = Unspecified () module-attribute \u00b6 An unspecified value when None is not appropriate Utility methods \u00b6 pydantictornado . util . apply_default ( value , default ) \u00b6 Use a default value when appropriate Note that this function will return value if default is None , ... , or an Unspecified instance. This matters when both value and default are one of the \"defaulted\" values (eg, None , ... , UNSPECIFIED ). Source code in src/pydantictornado/util.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def apply_default ( value : T , default : T | Unspecified | None | types . EllipsisType , ) -> T : \"\"\"Use a default value when appropriate Note that this function will return `value` if `default` is `None`, `...`, or an `Unspecified` instance. This matters when *both* `value` and `default` are one of the \"defaulted\" values (eg, `None`, `...`, `UNSPECIFIED`). \"\"\" if default in ( UNSPECIFIED , None , ... ) or isinstance ( default , Unspecified ): return value if value in ( UNSPECIFIED , None , ... ) or isinstance ( value , Unspecified ): return typing . cast ( T , default ) return value pydantictornado . util . json_serialize_hook ( obj ) \u00b6 Standard default function passed to json.dump This function is used to serialize a number of non-standard objects in response values. Type Description has attribute isoformat obj.isoformat() ipaddress.IPv4Address str(obj) ipaddress.IPv6Address str(obj) uuid.UUID str(obj) yarl.URL str(obj) datetime.timedelta serialized as ISO-8601 duration Pydantic models obj.model_dump(by_alias=True) Source code in src/pydantictornado/util.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def json_serialize_hook ( obj : object , ) -> bool | float | int | str | dict [ str , object ]: \"\"\"Standard `default` function passed to json.dump This function is used to serialize a number of non-standard objects in response values. | Type | Description | | ------------------------- | ------------------------------- | | has attribute isoformat | `obj.isoformat()` | | [ipaddress.IPv4Address][] | `str(obj)` | | [ipaddress.IPv6Address][] | `str(obj)` | | [uuid.UUID][] | `str(obj)` | | [yarl.URL][] | `str(obj)` | | [datetime.timedelta][] | serialized as ISO-8601 duration | | Pydantic models | `obj.model_dump(by_alias=True)` | \"\"\" if isinstance ( obj , HasIsoFormat ): return obj . isoformat () if isinstance ( obj , ipaddress . IPv4Address | ipaddress . IPv6Address | uuid . UUID | yarl . URL , ): return str ( obj ) if isinstance ( obj , datetime . timedelta ): return _format_isoduration ( obj . total_seconds ()) if isinstance ( obj , pydantic . BaseModel ): return obj . model_dump ( by_alias = True ) raise errors . NotSerializableError ( obj ) Parsing utilities \u00b6 pydantictornado . util . convert_bool ( value ) \u00b6 Convert value into a Boolean based on configuration This function uses the pydantictornado.util.BOOLEAN_TRUE_STRINGS and pydantictornado.util.BOOLEAN_FALSE_STRINGS constants to convert value to a bool . If there is not a direct string match, it tries to convert value to an int and then casts that to a Boolean value. Raises pydantictornado.errors.ValueParseError if it cannot convert value to a Boolean value. Source code in src/pydantictornado/util.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def convert_bool ( value : str ) -> bool : \"\"\"Convert `value` into a Boolean based on configuration This function uses the [pydantictornado.util.BOOLEAN_TRUE_STRINGS][] and [pydantictornado.util.BOOLEAN_FALSE_STRINGS][] constants to convert `value` to a `bool`. If there is not a direct string match, it tries to convert `value` to an int and then casts that to a Boolean value. Raises [pydantictornado.errors.ValueParseError][] if it cannot convert `value` to a Boolean value. \"\"\" if value in BOOLEAN_TRUE_STRINGS : return True if value in BOOLEAN_FALSE_STRINGS : return False try : int_value = int ( value , base = 10 ) except ( TypeError , ValueError ): raise errors . ValueParseError ( value , int ) from None return bool ( int_value ) pydantictornado . util . parse_date ( value ) \u00b6 Parses value as a datetime.datetime and discards the time Source code in src/pydantictornado/util.py 347 348 349 def parse_date ( value : str ) -> datetime . date : \"\"\"Parses `value` as a datetime.datetime and discards the time\"\"\" return parse_datetime ( value ) . date () pydantictornado . util . parse_datetime ( value ) \u00b6 Parse value into a datetime according to ISO-8601 Uses datetime.datetime.fromisoformat to parse value . If that fails, then the shortened date forms are used. Raises pydantictornado.errors.ValueParseError if it cannot convert value to a Boolean value. Source code in src/pydantictornado/util.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def parse_datetime ( value : str ) -> datetime . datetime : \"\"\"Parse `value` into a datetime according to ISO-8601 Uses [datetime.datetime.fromisoformat][] to parse `value`. If that fails, then the shortened date forms are used. Raises [pydantictornado.errors.ValueParseError][] if it cannot convert `value` to a Boolean value. \"\"\" formats = [ '%Y' , '%Y-%m' , '%Y%m' ] with contextlib . suppress ( ValueError ): then = datetime . datetime . fromisoformat ( value ) if not then . tzinfo : then = then . replace ( tzinfo = datetime . UTC ) return then for fmt in formats : with contextlib . suppress ( ValueError ): return datetime . datetime . strptime ( value , fmt ) . replace ( tzinfo = datetime . UTC ) raise errors . ValueParseError ( value , datetime . datetime ) pydantictornado . util . BOOLEAN_FALSE_STRINGS : set [ str ] = set () module-attribute \u00b6 String values that are parsed as False for boolean parameters pydantictornado . util . BOOLEAN_TRUE_STRINGS : set [ str ] = set () module-attribute \u00b6 String values that are parsed as True for boolean parameters Root Errors \u00b6 pydantictornado.errors.PydanticTornadoError \u00b6 Bases: Exception Root of all errors raised by this library Source code in src/pydantictornado/errors.py 4 5 class PydanticTornadoError ( Exception ): \"\"\"Root of all errors raised by this library\"\"\" pydantictornado.errors.ConfigurationError \u00b6 Bases: PydanticTornadoError Root of initialization errors This category of errors occurs during route creation. Source code in src/pydantictornado/errors.py 38 39 40 41 42 43 class ConfigurationError ( PydanticTornadoError ): \"\"\"Root of initialization errors This category of errors occurs during route creation. \"\"\" Runtime errors \u00b6 pydantictornado.errors.TypeRequiredError \u00b6 Bases: PydanticTornadoError , TypeError A non-type was given where a type was required If you are seeing this, then you probably used a special form where a type is required. Source code in src/pydantictornado/errors.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class TypeRequiredError ( PydanticTornadoError , TypeError ): \"\"\"A non-type was given where a type was required If you are seeing this, then you probably used a [special form] where a [type] is required. [special form]: https://docs.python.org/3/library/typing.html#special-forms [type]: https://docs.python.org/3/library/typing.html#the-type-of-class-objects \"\"\" def __init__ ( self , value : object ) -> None : cls = type ( value ) super () . __init__ ( f 'Type required - { value !r} is a { cls . __module__ } . { cls . __name__ } ' ) self . value = value pydantictornado.errors.NotSerializableError \u00b6 Bases: PydanticTornadoError , TypeError Value is not a serializable type Source code in src/pydantictornado/errors.py 27 28 29 30 31 32 33 34 35 class NotSerializableError ( PydanticTornadoError , TypeError ): \"\"\"Value is not a serializable type\"\"\" def __init__ ( self , value : object ) -> None : super () . __init__ ( f 'Object of type { value . __class__ . __name__ } ' f 'is not serializable' ) self . value = value pydantictornado.errors.ValueParseError \u00b6 Bases: PydanticTornadoError , ValueError Failed to parse a request parameter value according to its type Source code in src/pydantictornado/errors.py 70 71 72 73 74 75 76 class ValueParseError ( PydanticTornadoError , ValueError ): \"\"\"Failed to parse a request parameter value according to its type\"\"\" def __init__ ( self , value : object , parser : object ) -> None : super () . __init__ ( f 'failed to parse { value !r} as { parser } ' ) self . expected_type = parser self . value = value Initialization errors \u00b6 pydantictornado.errors.UnroutableParameterTypeError \u00b6 Bases: PydanticTornadoError , TypeError Unhandled type annotation on a request method Source code in src/pydantictornado/errors.py 46 47 48 49 50 51 52 class UnroutableParameterTypeError ( PydanticTornadoError , TypeError ): \"\"\"Unhandled type annotation on a request method\"\"\" def __init__ ( self , cls : type ) -> None : super () . __init__ ( f 'Type { cls . __name__ } is not recognized by { self . __module__ } ' ) pydantictornado.errors.CoroutineRequiredError \u00b6 Bases: ConfigurationError , ValueError Source code in src/pydantictornado/errors.py 55 56 57 58 59 60 class CoroutineRequiredError ( ConfigurationError , ValueError ): def __init__ ( self , value : object ) -> None : value_description = getattr ( value , '__name__' , value ) super () . __init__ ( f ' { value_description !r} is a { type ( value ) } , expected a coroutine' ) pydantictornado.errors.NoHttpMethodsDefinedError \u00b6 Bases: ConfigurationError Route does not have a handler defined Source code in src/pydantictornado/errors.py 63 64 65 66 67 class NoHttpMethodsDefinedError ( ConfigurationError ): \"\"\"Route does not have a handler defined\"\"\" def __init__ ( self ) -> None : super () . __init__ ( 'At least one HTTP method implementation is required' ) Type annotations \u00b6 pydantictornado . request_handling . ReturnsNone = typing . Annotated [ None , 'ReturnsNone' ] module-attribute \u00b6 Explicitly mark functions that return None as a value This is used to distinguish between explicitly returning None versus implicitly returning None with an empty return. pydantictornado . request_handling . RequestMethod = typing . Callable [ ... , typing . Awaitable [ ResponseType ]] module-attribute \u00b6","title":"Programming interface"},{"location":"reference/api/#application-interface","text":"","title":"Application interface"},{"location":"reference/api/#pydantictornado.routing.Route","text":"Bases: URLSpec Source code in src/pydantictornado/routing.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 class Route ( tornado . routing . URLSpec ): _implementations : dict [ str , request_handling . RequestMethod ] def __init__ ( self , pattern : str | re . Pattern [ str ], ** kwargs : request_handling . RequestMethod | object , ) -> None : self . _implementations = {} target_kwargs = {} for name , value in kwargs . items (): if name . upper () in HTTP_METHOD_NAMES : if not util . is_coroutine_function ( value ): raise errors . CoroutineRequiredError ( value ) self . _implementations [ name . upper ()] = value target_kwargs [ name ] = value if not self . _implementations : raise errors . NoHttpMethodsDefinedError if isinstance ( pattern , str ): pattern = re . compile ( pattern . removesuffix ( '$' ) + '$' ) path_types : dict [ str , Converter ] = {} path_groups = pattern . groupindex if path_groups : target_kwargs [ 'path_types' ] = path_types for impl in self . _implementations . values (): _process_path_parameters ( impl , pattern , path_types ) super () . __init__ ( pattern , handler = request_handling . RequestHandler , kwargs = target_kwargs , ) @property def implementations ( self , ) -> typing . Iterator [ tuple [ str , request_handling . RequestMethod ]]: yield from self . _implementations . items ()","title":"Route"},{"location":"reference/api/#pydantictornado.request_handling.RequestHandler","text":"Bases: RequestHandler Source code in src/pydantictornado/request_handling.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class RequestHandler ( web . RequestHandler ): implementations : dict [ str , RequestMethod ] logger : logging . Logger __initialization_failure : Exception | None __path_coercions : dict [ str , PathCoercion ] # The following annotations help out type checkers. The # methods are bound in `initialize()`. delete : RequestMethod # type: ignore[assignment] # signature mismatch get : RequestMethod # type: ignore[assignment] # signature mismatch head : RequestMethod # type: ignore[assignment] # signature mismatch options : RequestMethod # type: ignore[assignment] # signature mismatch patch : RequestMethod # type: ignore[assignment] # signature mismatch post : RequestMethod # type: ignore[assignment] # signature mismatch put : RequestMethod # type: ignore[assignment] # signature mismatch def initialize ( self , * , path_types : dict [ str , PathCoercion ] | None = None , ** kwargs : object , ) -> None : self . __initialization_failure = None self . logger = util . get_logger_for ( self ) self . implementations = {} self . __path_coercions = collections . defaultdict ( identity_transform_factory ) if path_types : self . __path_coercions . update ( path_types ) for http_method in self . SUPPORTED_METHODS : key = http_method . lower () func = kwargs . pop ( key , util . UNSPECIFIED ) if func is not util . UNSPECIFIED : if not util . is_coroutine_function ( func ): self . logger . critical ( 'implementation method for %r is not a co-routine' , key ) self . __initialization_failure = web . HTTPError ( 500 ) self . implementations [ http_method ] = typing . cast ( RequestMethod , func ) setattr ( self , key , self . _handle_request ) self . SUPPORTED_METHODS = tuple ( self . implementations . keys ()) # type: ignore[assignment] super () . initialize ( ** kwargs ) async def _handle_request ( self , ** path_kwargs : str ) -> None : if self . __initialization_failure is not None : raise self . __initialization_failure if self . request . method is None : raise web . HTTPError ( 500 ) # pragma: nocover -- should not happen! func = self . implementations [ self . request . method ] sig = inspect . signature ( func , eval_str = True ) try : kwargs = { name : self . __path_coercions [ name ]( value ) for name , value in path_kwargs . items () } except ValueError as error : self . logger . error ( 'failed to process path parameter for %s : %s ' , func , error ) raise web . HTTPError ( 400 ) from None self . __handle_injections ( sig . parameters , kwargs ) result = await func ( ** kwargs ) if result is not None or sig . return_annotation == ReturnsNone : self . send_response ( result ) def send_response ( self , body : ResponseType | None , * _args : object , ** _kwargs : object ) -> None : self . set_header ( 'content-type' , 'application/json; charset=\"UTF-8\"' ) self . write ( json . dumps ( body , default = util . json_serialize_hook ) . encode ( 'utf-8' ) ) def __handle_injections ( self , annotations : collections . abc . Mapping [ str , inspect . Parameter ], kwargs : dict [ str , object ], ) -> None : logger = util . get_logger_for ( self ) mapping = util . ClassMapping [ object ]( { tornado . httputil . HTTPServerRequest : self . request , tornado . web . Application : self . application , tornado . web . RequestHandler : self , } ) for name , param in annotations . items (): logger . debug ( 'processing %s -> annotation= %r origin= %r ' , name , param . annotation , typing . get_origin ( param . annotation ), ) if typing . get_origin ( param . annotation ) is None : if issubclass ( param . annotation , pydantic . BaseModel ): kwargs [ name ] = param . annotation . model_validate_json ( self . request . body ) else : value = mapping . get ( param . annotation , util . UNSPECIFIED ) if value is not util . UNSPECIFIED : kwargs [ name ] = value logger . debug ( 'kwargs[ %s ] <- %r ' , name , kwargs . get ( name , util . Unspecified ()) )","title":"RequestHandler"},{"location":"reference/api/#useful-utilities","text":"","title":"Useful utilities"},{"location":"reference/api/#types-and-type-helpers","text":"","title":"Types and type helpers"},{"location":"reference/api/#pydantictornado.util.FieldOmittingMixin","text":"Bases: BaseModel Mix this into pydantic models to omit None fields The fields named in OMIT_IF_NONE will be ignored during serialization if their value is None . The fields named in OMIT_IF_EMPTY will be ignored during serialization if their value is empty . This is only relevant for instances that implement collections.abc.Sized . Source code in src/pydantictornado/util.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class FieldOmittingMixin ( pydantic . BaseModel ): \"\"\"Mix this into pydantic models to omit `None` fields The fields named in `OMIT_IF_NONE` will be ignored during serialization if their value is `None`. The fields named in `OMIT_IF_EMPTY` will be ignored during serialization if their value is *empty*. This is only relevant for instances that implement [collections.abc.Sized][]. \"\"\" OMIT_IF_EMPTY : typing . ClassVar [ tuple [ str , ... ]] = () OMIT_IF_NONE : typing . ClassVar [ tuple [ str , ... ]] = () @pydantic . model_serializer ( mode = 'wrap' ) def _omit_fields ( self , handler : typing . Callable [ [ typing . Self , pydantic . SerializationInfo ], dict [ str , object ] ], info : pydantic . SerializationInfo , ) -> dict [ str , object ]: result = handler ( self , info ) for name in self . model_fields : value = result . get ( name , None ) empty = isinstance ( value , collections . abc . Sized ) and not len ( value ) if ( name in self . OMIT_IF_NONE and value is None ) or ( name in self . OMIT_IF_EMPTY and empty ): result . pop ( name ) return result","title":"FieldOmittingMixin"},{"location":"reference/api/#pydantictornado.util.HasIsoFormat","text":"Bases: Protocol Runtime checkable protocol that detects the isoformat method Source code in src/pydantictornado/util.py 259 260 261 262 263 264 @typing . runtime_checkable class HasIsoFormat ( typing . Protocol ): \"\"\"Runtime checkable protocol that detects the isoformat method\"\"\" def isoformat ( self , spec : str = ... ) -> str : ...","title":"HasIsoFormat"},{"location":"reference/api/#pydantictornado.util.Unspecified","text":"Simple type that is never true Source code in src/pydantictornado/util.py 30 31 32 33 34 35 36 37 class Unspecified : \"\"\"Simple type that is never true\"\"\" def __bool__ ( self ) -> bool : # pragma: nocover # note that this method may or may not be necessary based # on the implementation ... I'm leaving it here to avoid # adding and removing it constantly return False","title":"Unspecified"},{"location":"reference/api/#pydantictornado.util.UNSPECIFIED","text":"An unspecified value when None is not appropriate","title":"UNSPECIFIED"},{"location":"reference/api/#utility-methods","text":"","title":"Utility methods"},{"location":"reference/api/#pydantictornado.util.apply_default","text":"Use a default value when appropriate Note that this function will return value if default is None , ... , or an Unspecified instance. This matters when both value and default are one of the \"defaulted\" values (eg, None , ... , UNSPECIFIED ). Source code in src/pydantictornado/util.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def apply_default ( value : T , default : T | Unspecified | None | types . EllipsisType , ) -> T : \"\"\"Use a default value when appropriate Note that this function will return `value` if `default` is `None`, `...`, or an `Unspecified` instance. This matters when *both* `value` and `default` are one of the \"defaulted\" values (eg, `None`, `...`, `UNSPECIFIED`). \"\"\" if default in ( UNSPECIFIED , None , ... ) or isinstance ( default , Unspecified ): return value if value in ( UNSPECIFIED , None , ... ) or isinstance ( value , Unspecified ): return typing . cast ( T , default ) return value","title":"apply_default"},{"location":"reference/api/#pydantictornado.util.json_serialize_hook","text":"Standard default function passed to json.dump This function is used to serialize a number of non-standard objects in response values. Type Description has attribute isoformat obj.isoformat() ipaddress.IPv4Address str(obj) ipaddress.IPv6Address str(obj) uuid.UUID str(obj) yarl.URL str(obj) datetime.timedelta serialized as ISO-8601 duration Pydantic models obj.model_dump(by_alias=True) Source code in src/pydantictornado/util.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def json_serialize_hook ( obj : object , ) -> bool | float | int | str | dict [ str , object ]: \"\"\"Standard `default` function passed to json.dump This function is used to serialize a number of non-standard objects in response values. | Type | Description | | ------------------------- | ------------------------------- | | has attribute isoformat | `obj.isoformat()` | | [ipaddress.IPv4Address][] | `str(obj)` | | [ipaddress.IPv6Address][] | `str(obj)` | | [uuid.UUID][] | `str(obj)` | | [yarl.URL][] | `str(obj)` | | [datetime.timedelta][] | serialized as ISO-8601 duration | | Pydantic models | `obj.model_dump(by_alias=True)` | \"\"\" if isinstance ( obj , HasIsoFormat ): return obj . isoformat () if isinstance ( obj , ipaddress . IPv4Address | ipaddress . IPv6Address | uuid . UUID | yarl . URL , ): return str ( obj ) if isinstance ( obj , datetime . timedelta ): return _format_isoduration ( obj . total_seconds ()) if isinstance ( obj , pydantic . BaseModel ): return obj . model_dump ( by_alias = True ) raise errors . NotSerializableError ( obj )","title":"json_serialize_hook"},{"location":"reference/api/#parsing-utilities","text":"","title":"Parsing utilities"},{"location":"reference/api/#pydantictornado.util.convert_bool","text":"Convert value into a Boolean based on configuration This function uses the pydantictornado.util.BOOLEAN_TRUE_STRINGS and pydantictornado.util.BOOLEAN_FALSE_STRINGS constants to convert value to a bool . If there is not a direct string match, it tries to convert value to an int and then casts that to a Boolean value. Raises pydantictornado.errors.ValueParseError if it cannot convert value to a Boolean value. Source code in src/pydantictornado/util.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def convert_bool ( value : str ) -> bool : \"\"\"Convert `value` into a Boolean based on configuration This function uses the [pydantictornado.util.BOOLEAN_TRUE_STRINGS][] and [pydantictornado.util.BOOLEAN_FALSE_STRINGS][] constants to convert `value` to a `bool`. If there is not a direct string match, it tries to convert `value` to an int and then casts that to a Boolean value. Raises [pydantictornado.errors.ValueParseError][] if it cannot convert `value` to a Boolean value. \"\"\" if value in BOOLEAN_TRUE_STRINGS : return True if value in BOOLEAN_FALSE_STRINGS : return False try : int_value = int ( value , base = 10 ) except ( TypeError , ValueError ): raise errors . ValueParseError ( value , int ) from None return bool ( int_value )","title":"convert_bool"},{"location":"reference/api/#pydantictornado.util.parse_date","text":"Parses value as a datetime.datetime and discards the time Source code in src/pydantictornado/util.py 347 348 349 def parse_date ( value : str ) -> datetime . date : \"\"\"Parses `value` as a datetime.datetime and discards the time\"\"\" return parse_datetime ( value ) . date ()","title":"parse_date"},{"location":"reference/api/#pydantictornado.util.parse_datetime","text":"Parse value into a datetime according to ISO-8601 Uses datetime.datetime.fromisoformat to parse value . If that fails, then the shortened date forms are used. Raises pydantictornado.errors.ValueParseError if it cannot convert value to a Boolean value. Source code in src/pydantictornado/util.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def parse_datetime ( value : str ) -> datetime . datetime : \"\"\"Parse `value` into a datetime according to ISO-8601 Uses [datetime.datetime.fromisoformat][] to parse `value`. If that fails, then the shortened date forms are used. Raises [pydantictornado.errors.ValueParseError][] if it cannot convert `value` to a Boolean value. \"\"\" formats = [ '%Y' , '%Y-%m' , '%Y%m' ] with contextlib . suppress ( ValueError ): then = datetime . datetime . fromisoformat ( value ) if not then . tzinfo : then = then . replace ( tzinfo = datetime . UTC ) return then for fmt in formats : with contextlib . suppress ( ValueError ): return datetime . datetime . strptime ( value , fmt ) . replace ( tzinfo = datetime . UTC ) raise errors . ValueParseError ( value , datetime . datetime )","title":"parse_datetime"},{"location":"reference/api/#pydantictornado.util.BOOLEAN_FALSE_STRINGS","text":"String values that are parsed as False for boolean parameters","title":"BOOLEAN_FALSE_STRINGS"},{"location":"reference/api/#pydantictornado.util.BOOLEAN_TRUE_STRINGS","text":"String values that are parsed as True for boolean parameters","title":"BOOLEAN_TRUE_STRINGS"},{"location":"reference/api/#root-errors","text":"","title":"Root Errors"},{"location":"reference/api/#pydantictornado.errors.PydanticTornadoError","text":"Bases: Exception Root of all errors raised by this library Source code in src/pydantictornado/errors.py 4 5 class PydanticTornadoError ( Exception ): \"\"\"Root of all errors raised by this library\"\"\"","title":"PydanticTornadoError"},{"location":"reference/api/#pydantictornado.errors.ConfigurationError","text":"Bases: PydanticTornadoError Root of initialization errors This category of errors occurs during route creation. Source code in src/pydantictornado/errors.py 38 39 40 41 42 43 class ConfigurationError ( PydanticTornadoError ): \"\"\"Root of initialization errors This category of errors occurs during route creation. \"\"\"","title":"ConfigurationError"},{"location":"reference/api/#runtime-errors","text":"","title":"Runtime errors"},{"location":"reference/api/#pydantictornado.errors.TypeRequiredError","text":"Bases: PydanticTornadoError , TypeError A non-type was given where a type was required If you are seeing this, then you probably used a special form where a type is required. Source code in src/pydantictornado/errors.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class TypeRequiredError ( PydanticTornadoError , TypeError ): \"\"\"A non-type was given where a type was required If you are seeing this, then you probably used a [special form] where a [type] is required. [special form]: https://docs.python.org/3/library/typing.html#special-forms [type]: https://docs.python.org/3/library/typing.html#the-type-of-class-objects \"\"\" def __init__ ( self , value : object ) -> None : cls = type ( value ) super () . __init__ ( f 'Type required - { value !r} is a { cls . __module__ } . { cls . __name__ } ' ) self . value = value","title":"TypeRequiredError"},{"location":"reference/api/#pydantictornado.errors.NotSerializableError","text":"Bases: PydanticTornadoError , TypeError Value is not a serializable type Source code in src/pydantictornado/errors.py 27 28 29 30 31 32 33 34 35 class NotSerializableError ( PydanticTornadoError , TypeError ): \"\"\"Value is not a serializable type\"\"\" def __init__ ( self , value : object ) -> None : super () . __init__ ( f 'Object of type { value . __class__ . __name__ } ' f 'is not serializable' ) self . value = value","title":"NotSerializableError"},{"location":"reference/api/#pydantictornado.errors.ValueParseError","text":"Bases: PydanticTornadoError , ValueError Failed to parse a request parameter value according to its type Source code in src/pydantictornado/errors.py 70 71 72 73 74 75 76 class ValueParseError ( PydanticTornadoError , ValueError ): \"\"\"Failed to parse a request parameter value according to its type\"\"\" def __init__ ( self , value : object , parser : object ) -> None : super () . __init__ ( f 'failed to parse { value !r} as { parser } ' ) self . expected_type = parser self . value = value","title":"ValueParseError"},{"location":"reference/api/#initialization-errors","text":"","title":"Initialization errors"},{"location":"reference/api/#pydantictornado.errors.UnroutableParameterTypeError","text":"Bases: PydanticTornadoError , TypeError Unhandled type annotation on a request method Source code in src/pydantictornado/errors.py 46 47 48 49 50 51 52 class UnroutableParameterTypeError ( PydanticTornadoError , TypeError ): \"\"\"Unhandled type annotation on a request method\"\"\" def __init__ ( self , cls : type ) -> None : super () . __init__ ( f 'Type { cls . __name__ } is not recognized by { self . __module__ } ' )","title":"UnroutableParameterTypeError"},{"location":"reference/api/#pydantictornado.errors.CoroutineRequiredError","text":"Bases: ConfigurationError , ValueError Source code in src/pydantictornado/errors.py 55 56 57 58 59 60 class CoroutineRequiredError ( ConfigurationError , ValueError ): def __init__ ( self , value : object ) -> None : value_description = getattr ( value , '__name__' , value ) super () . __init__ ( f ' { value_description !r} is a { type ( value ) } , expected a coroutine' )","title":"CoroutineRequiredError"},{"location":"reference/api/#pydantictornado.errors.NoHttpMethodsDefinedError","text":"Bases: ConfigurationError Route does not have a handler defined Source code in src/pydantictornado/errors.py 63 64 65 66 67 class NoHttpMethodsDefinedError ( ConfigurationError ): \"\"\"Route does not have a handler defined\"\"\" def __init__ ( self ) -> None : super () . __init__ ( 'At least one HTTP method implementation is required' )","title":"NoHttpMethodsDefinedError"},{"location":"reference/api/#type-annotations","text":"","title":"Type annotations"},{"location":"reference/api/#pydantictornado.request_handling.ReturnsNone","text":"Explicitly mark functions that return None as a value This is used to distinguish between explicitly returning None versus implicitly returning None with an empty return.","title":"ReturnsNone"},{"location":"reference/api/#pydantictornado.request_handling.RequestMethod","text":"","title":"RequestMethod"},{"location":"users-guide/examples/","text":"Sometimes it is easiest to see how something is done rather than being told how to do it. We're going to start with that approach... the following sections show how to do various things in separate request handling functions. Here is the application file that each request handler is added to. import signal import asyncio import contextlib import logging from pydantictornado import routing from tornado import web async def handler (): ... async def main () -> None : running = asyncio . Event () asyncio . get_running_loop () . add_signal_handler ( signal . SIGINT , running . set ) app = web . Application ([ routing . Route ( '/' , get = handler ), ]) app . listen ( 8888 ) with contextlib . suppress ( KeyboardInterrupt ): await running . wait () if __name__ == '__main__' : logging . basicConfig ( level = logging . INFO , format = ' %(levelname)1.1s %(name)s : %(message)s ' ) Simple route returning a JSON object \u00b6 Whatever a request handler returns is passed to tornado.web.RequestHandler.write after serializing it with a slightly extended json.dumps . import datetime async def handler () -> dict [ str , datetime . datetime ]: return { 'now' : datetime . datetime . now ( datetime . UTC )} Note The return type annotation is not currently used but it will be in the future! GET / HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 200 OK Content-Type : application/json; charset=UTF-8 Content-Length : 43 { \"now\" : \"2024-03-03T20:25:50.286858+00:00\" } Setting a status code \u00b6 Tornado uses a method on the RequestHandler to set the response status code, so you need to request access to it. Simply include any parameter with a type annotation of tornado.web.RequestHandler . When your function is called, it will be passed an instance to the request handler that received the request. from tornado import web async def handler ( h : web . RequestHandler ) -> None : h . set_status ( 204 ) GET / HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 204 No Content The parameter can be named whatever you want and can appear anywhere in the parameter list. In fact, any parameter annotated with RequestHandler will receive the same value. You can even include a default value if you want -- this can be useful if you want to use the same function for more than one HTTP resource. Note This usage of type annotations is used to inject access to various framework objects into your request handlers. Many thanks to FastAPI for introducing me to this technique. Processing path parameters \u00b6 This is almost identical to how path parameters work in vanilla Tornado. The difference is that parameters are converted to Python objects instead of passing strings. import uuid async def handler ( item_id : uuid . UUID ) -> dict [ str , object ]: return { 'value' : item_id } GET /00000000-0000-0000-0000-000000000000 HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 200 OK Content-Type : application/json; charset=UTF-8 Content-Length : 46 { \"id\" : \"00000000-0000-0000-0000-000000000000\" } Of course, you will have to add the variable to the path expression in the route.Route line. app = web.Application([ - routing.Route('/', get=handler), + routing.Route('/(?P<item_id>.*)', get=handler), ]) That looks almost identical to the Tornado handler other than the type exception. Let's see how it differs by requesting /not-a-uuid : HTTP / 1.1 400 Bad Request Content-Type : text/html; charset=UTF-8 Content-Length : 73 < html >< title > 400: Bad Request </ title >< body > 400: Bad Request </ body ></ html > Your request handler is never invoked in this case since the type conversion fails. The error reporting defaults to Tornado's standard behaviour of returning an HTML document. We will come back to this later.","title":"Examples"},{"location":"users-guide/examples/#simple-route-returning-a-json-object","text":"Whatever a request handler returns is passed to tornado.web.RequestHandler.write after serializing it with a slightly extended json.dumps . import datetime async def handler () -> dict [ str , datetime . datetime ]: return { 'now' : datetime . datetime . now ( datetime . UTC )} Note The return type annotation is not currently used but it will be in the future! GET / HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 200 OK Content-Type : application/json; charset=UTF-8 Content-Length : 43 { \"now\" : \"2024-03-03T20:25:50.286858+00:00\" }","title":"Simple route returning a JSON object"},{"location":"users-guide/examples/#setting-a-status-code","text":"Tornado uses a method on the RequestHandler to set the response status code, so you need to request access to it. Simply include any parameter with a type annotation of tornado.web.RequestHandler . When your function is called, it will be passed an instance to the request handler that received the request. from tornado import web async def handler ( h : web . RequestHandler ) -> None : h . set_status ( 204 ) GET / HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 204 No Content The parameter can be named whatever you want and can appear anywhere in the parameter list. In fact, any parameter annotated with RequestHandler will receive the same value. You can even include a default value if you want -- this can be useful if you want to use the same function for more than one HTTP resource. Note This usage of type annotations is used to inject access to various framework objects into your request handlers. Many thanks to FastAPI for introducing me to this technique.","title":"Setting a status code"},{"location":"users-guide/examples/#processing-path-parameters","text":"This is almost identical to how path parameters work in vanilla Tornado. The difference is that parameters are converted to Python objects instead of passing strings. import uuid async def handler ( item_id : uuid . UUID ) -> dict [ str , object ]: return { 'value' : item_id } GET /00000000-0000-0000-0000-000000000000 HTTP / 1.1 Host : 127.0.0.1:8888 Accept : */* HTTP / 1.1 200 OK Content-Type : application/json; charset=UTF-8 Content-Length : 46 { \"id\" : \"00000000-0000-0000-0000-000000000000\" } Of course, you will have to add the variable to the path expression in the route.Route line. app = web.Application([ - routing.Route('/', get=handler), + routing.Route('/(?P<item_id>.*)', get=handler), ]) That looks almost identical to the Tornado handler other than the type exception. Let's see how it differs by requesting /not-a-uuid : HTTP / 1.1 400 Bad Request Content-Type : text/html; charset=UTF-8 Content-Length : 73 < html >< title > 400: Bad Request </ title >< body > 400: Bad Request </ body ></ html > Your request handler is never invoked in this case since the type conversion fails. The error reporting defaults to Tornado's standard behaviour of returning an HTML document. We will come back to this later.","title":"Processing path parameters"},{"location":"users-guide/injections/","text":"In vanilla Tornado, a tornado.web.RequestHandler has methods to communicate with the client. The request information is implicitly available as self.request . You send response headers with self.add_header() , set the response code with self.set_status() , etc. This functionality is lost when we move away from a handler class. This library uses type annotations on parameters as tags that inject access to the tornado.web.Application and tornado.web.RequestHandler instances. See Rely on Python type annotations for the rationale behind the decision. Request information \u00b6 Path parameters \u00b6 Path parameters are identified as keyword parameters by name. The values are converted from strings to Python values based on the type annotation. Request body \u00b6 You can access a deserialized version of the request body by adding a parameter that is typed as a Pydantic object. Warning All parameters that are typed as pydantic.BaseModel subclasses are deserialized. If you include multiple model parameters, then the request body is deserialized multiple times and must match each model type. You can also access the request by injecting a tornado.httputil.HTTPServerRequest parameter and using the body property. Everything else \u00b6 Annotation Description tornado.httputil.HTTPServerRequest The request being processed tornado.web.Application The application attribute from the request handler tornado.web.RequestHandler The tornado.web.RequestHandler that is processing the request You can access most of the traditional Tornado interface by injecting a tornado.web.RequestHandler parameter. Supported parameter types \u00b6 Standard types \u00b6 Annotation Parsing details float Floating point numbers are parsed using the float constructor int Integer numbers are parsed using the int constructor as a decimal number (base 10) str String instances are passed through as-is uuid.UUID UUID instances are parsed using the uuid.UUID constructor with the string value datetime.datetime Date-times are parsed using datetime.datetime.fromisoformat datetime.date Dates are parsed using datetime.datetime.fromisoformat and a few hard-coded format strings to come very close to the ISO-8601 standard ipaddress.IPv4Address IPv4 addresses are parsed by calling the ipaddress.IPv4Address constructor with the string value ipaddress.IPv6Address IPv6 addresses are parsed by calling the ipaddress.IPv6Address constructor with the string value Boolean values \u00b6 Boolean values are problematic when it comes to internationalization. Many libraries omit support for them in query parameters for this reason. I took a slightly different approach. Boolean values are parsed as integers and anything non-zero is converted to True . This is probably surprising and likely doesn't work for your application. That's fine. There are two global set[str] values that you can modify to set which strings are considered truthy and falsy . pydantictornado . util . BOOLEAN_FALSE_STRINGS : set [ str ] = set () module-attribute \u00b6 String values that are parsed as False for boolean parameters pydantictornado . util . BOOLEAN_TRUE_STRINGS : set [ str ] = set () module-attribute \u00b6 String values that are parsed as True for boolean parameters Warning I suspect that these will be moved into a configuration object at some point in the future.","title":"Injecting information"},{"location":"users-guide/injections/#request-information","text":"","title":"Request information"},{"location":"users-guide/injections/#path-parameters","text":"Path parameters are identified as keyword parameters by name. The values are converted from strings to Python values based on the type annotation.","title":"Path parameters"},{"location":"users-guide/injections/#request-body","text":"You can access a deserialized version of the request body by adding a parameter that is typed as a Pydantic object. Warning All parameters that are typed as pydantic.BaseModel subclasses are deserialized. If you include multiple model parameters, then the request body is deserialized multiple times and must match each model type. You can also access the request by injecting a tornado.httputil.HTTPServerRequest parameter and using the body property.","title":"Request body"},{"location":"users-guide/injections/#everything-else","text":"Annotation Description tornado.httputil.HTTPServerRequest The request being processed tornado.web.Application The application attribute from the request handler tornado.web.RequestHandler The tornado.web.RequestHandler that is processing the request You can access most of the traditional Tornado interface by injecting a tornado.web.RequestHandler parameter.","title":"Everything else"},{"location":"users-guide/injections/#supported-parameter-types","text":"","title":"Supported parameter types"},{"location":"users-guide/injections/#standard-types","text":"Annotation Parsing details float Floating point numbers are parsed using the float constructor int Integer numbers are parsed using the int constructor as a decimal number (base 10) str String instances are passed through as-is uuid.UUID UUID instances are parsed using the uuid.UUID constructor with the string value datetime.datetime Date-times are parsed using datetime.datetime.fromisoformat datetime.date Dates are parsed using datetime.datetime.fromisoformat and a few hard-coded format strings to come very close to the ISO-8601 standard ipaddress.IPv4Address IPv4 addresses are parsed by calling the ipaddress.IPv4Address constructor with the string value ipaddress.IPv6Address IPv6 addresses are parsed by calling the ipaddress.IPv6Address constructor with the string value","title":"Standard types"},{"location":"users-guide/injections/#boolean-values","text":"Boolean values are problematic when it comes to internationalization. Many libraries omit support for them in query parameters for this reason. I took a slightly different approach. Boolean values are parsed as integers and anything non-zero is converted to True . This is probably surprising and likely doesn't work for your application. That's fine. There are two global set[str] values that you can modify to set which strings are considered truthy and falsy .","title":"Boolean values"},{"location":"users-guide/injections/#pydantictornado.util.BOOLEAN_FALSE_STRINGS","text":"String values that are parsed as False for boolean parameters","title":"BOOLEAN_FALSE_STRINGS"},{"location":"users-guide/injections/#pydantictornado.util.BOOLEAN_TRUE_STRINGS","text":"String values that are parsed as True for boolean parameters Warning I suspect that these will be moved into a configuration object at some point in the future.","title":"BOOLEAN_TRUE_STRINGS"},{"location":"users-guide/writing-handlers/","text":"Request handlers are simply async functions decorated with type annotations . The annotations control how path parameters are converted from strings. Annotations are also used to inject the raw request object, the tornado.web.RequestHandler that is processing the request, and other supporting objects. See Injecting Information for the full list of supported injections . A lot of your handlers will look much like they do today except for the parts that interact with the request and response. Let's say you have a request handler that processes a POST request by inserting some data into a database and returning the newly created record. Here's an example without any of the pesky database stuff. import uuid import json from tornado import web class MyHandler ( web . RequestHandler ): async def post ( self ) -> None : body = json . loads ( self . request . body . decode ( 'utf-8' )) created_id = await self . _insert_new_thing ( self . application . db , body ) body [ 'id' ] = str ( created_id ) self . write ( body ) async def _insert_new_thing ( self , db , body ) -> uuid . UUID : row = await db . execute ( 'INSERT INTO things(name) VALUES (:name) RETURNING id' , name = body [ 'name' ]) return row [ 'id' ] class Application ( web . Application ): def __init__ ( self , ** settings ) -> None : super () . __init__ ( [ web . url ( r '/things' , MyHandler )], ** settings ) The reimplementation of the handler using pydantic for the request and response would look like the following: import uuid import pydantic from pydantictornado import routing from tornado import web class CreateRequest ( pydantic . BaseModel ): name : str class Thingy ( pydantic . BaseModel ): id : uuid . UUID name : str async def create_thingy ( details : CreateRequest , app : web . Application ) -> Thingy : return await insert_new_thing ( app . db , details ) async def insert_new_thing ( db , details : CreateRequest ) -> Thingy : row = await db . execute ( 'INSERT INTO things(name) VALUES (:name) RETURNING id' , name = details . name ) return Thingy ( id = row [ 'id' ], name = details . name ) class Application ( web . Application ): def __init__ ( self , ** settings ) -> None : super () . __init__ ( [ routing . Route ( r '/things' , post = create_thingy )], ** settings ) Yeah... once again... there is more code there. Here's what you've gained: nothing before the Application definition uses tornado directly the only thing that is a raw (untyped) dictionary is the row returned from the database layer testing your application code doesn't require a web stack ... your implementation consists of free functions The testing point is a little bit of a white lie. You do need to create application and request objects if your application code needs them. If you are only accessing properties, like the db property, then you can test with mocks that have the properties that you need only. After all, that is the most reliable way to ensure that your code does not rely on hidden functionality . Okay... what aren't you telling me? \u00b6 There are a number of things that I haven't worked out yet. One of the conventions in Tornado applications is to do useful things in your on_finish() method like update performance counters. We also insert error handling wrappers to convert uncaught exceptions to well-formed responses and the like. I still have to work through the details on this stuff... but this is pre-alpha software at this point! .","title":"Writing handlers"},{"location":"users-guide/writing-handlers/#okay-what-arent-you-telling-me","text":"There are a number of things that I haven't worked out yet. One of the conventions in Tornado applications is to do useful things in your on_finish() method like update performance counters. We also insert error handling wrappers to convert uncaught exceptions to well-formed responses and the like. I still have to work through the details on this stuff... but this is pre-alpha software at this point! .","title":"Okay... what aren't you telling me?"},{"location":"adr/","text":"I am trying to capture the important decisions as I write this library here so that they are not lost to the sands of time. Michael Nygard describes the need for this much better than I ever could in Documenting Architecture Decisions . The graph below shows the decisions and any relationships between them. graph TD adr0001-record-architecture-decisions[Record architecture decisions] click adr0001-record-architecture-decisions \"adr/0001-record-architecture-decisions/\" _blank adr0001-record-architecture-decisions:::mermaid-accepted adr0001-record-architecture-decisions:::mermaid-common adr0002-generate-documentation-using-mkdocs[Generate documentation using mkdocs] click adr0002-generate-documentation-using-mkdocs \"adr/0002-generate-documentation-using-mkdocs/\" _blank adr0002-generate-documentation-using-mkdocs:::mermaid-accepted adr0002-generate-documentation-using-mkdocs:::mermaid-common adr0003-type-annotations[Rely on Python type annotations] click adr0003-type-annotations \"adr/0003-type-annotations/\" _blank adr0003-type-annotations:::mermaid-accepted adr0003-type-annotations:::mermaid-common classDef mermaid-draft fill:#a3a3a3; classDef mermaid-common color:#595959; classDef mermaid-proposed fill:#b6d8ff; classDef mermaid-common color:#595959; classDef mermaid-accepted fill:#b4eda0; classDef mermaid-common color:#595959; classDef mermaid-rejected fill:#ffd5d1; classDef mermaid-common color:#595959; classDef mermaid-superseded fill:#ffebb6; classDef mermaid-common color:#595959;","title":"Architecture decisions"}]}